{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "advised-slave",
   "metadata": {},
   "source": [
    "# Введение в генеративные модели\n",
    "\n",
    "В данном ноутбуке будет рассмотрено несколько базовых подходов к генерации изображений с помощью сверточных нейросетей. Мы будем генерировать фотографии лиц. В качестве эталона, будем использовать выровненные, центрированные фотографии знаменитостей из датасета CelebFaces Attributes (CelebA).\n",
    "\n",
    "Установим нужные пакеты, скачаем датасет и чекпоинты моделей, которые нам понадобятся в этом задании."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-federation",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet matplotlib torchvision lightning-bolts gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pl_bolts as bolt\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from pl_bolts.models.autoencoders.components import resnet18_decoder, resnet18_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-resident",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Download, verify and extract the CelebA dataset\n",
    "    tv.datasets.CelebA(\".\", download=True)\n",
    "except Exception:\n",
    "    print(\"Official download method failed. Trying direct download from Google Drive.\")\n",
    "    !mkdir -p celeba\n",
    "    !gdown --id 1bddRLZ8KpwR_gOYRS_FcQ7FLrkFCs39L --output celeba/img_align_celeba.zip\n",
    "    !gdown --id 1k3O3jGswTH09UytdkoXSxblj6RzpdzQN --output celeba_txt.zip\n",
    "    !unzip -o celeba_txt.zip\n",
    "\n",
    "    print()\n",
    "    for _, md5sum, file_name in tv.datasets.CelebA.file_list:\n",
    "        print(\"Expected:  \", md5sum, \" celeba/\" + file_name)\n",
    "        print(\"Downloaded: \", end=\"\", flush=True)\n",
    "        !md5sum \"celeba/{file_name}\"\n",
    "        print()\n",
    "\n",
    "    # Verify and extract the CelebA dataset\n",
    "    tv.datasets.CelebA(\".\", download=True)\n",
    "\n",
    "# Download pretrained model checkpoints\n",
    "if not os.path.exists(\"AutoEncoder.ckpt\"):\n",
    "    !gdown --id 1nCNYSY0hHq6ZejLEIGO4Xy4k8wqZC084 --output AutoEncoder.ckpt\n",
    "if not os.path.exists(\"VariationalAutoEncoder.ckpt\"):\n",
    "    !gdown --id 1f04WZXkVokaotdDyyiTXJHtRvTl-qMdJ --output VariationalAutoEncoder.ckpt\n",
    "if not os.path.exists(\"GenerativeAdversarialNetwork.ckpt\"):\n",
    "    !gdown --id 1KZ_FtfHrwM0LCul3pdArhjnDzbwd7B_8 --output GenerativeAdversarialNetwork.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is mostly boilerplate code for working with the CelebA dataset.\n",
    "class CelebABoilerplate(pl.LightningModule):\n",
    "    def __init__(self, image_size, batch_size=64, lr=0.0001, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def get_dataset(self, kind):\n",
    "        return tv.datasets.CelebA(\n",
    "            \".\",\n",
    "            split=kind,\n",
    "            transform=tv.transforms.Compose([\n",
    "                tv.transforms.Resize(self.hparams.image_size),\n",
    "                tv.transforms.CenterCrop(self.hparams.image_size),\n",
    "                tv.transforms.ToTensor(),\n",
    "            ]),\n",
    "        )\n",
    "\n",
    "    def get_dataloader(self, kind):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.get_dataset(kind),\n",
    "            num_workers=os.cpu_count(),\n",
    "            shuffle=kind == \"train\",\n",
    "            batch_size=self.hparams.batch_size,\n",
    "        )\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.get_dataloader(\"train\")\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.get_dataloader(\"valid\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "def get_latest_version(root_dir):\n",
    "    versions = sorted(glob.glob(f\"{root_dir}/lightning_logs/version_*\"))\n",
    "    return versions[-1] if versions else None\n",
    "\n",
    "def get_latest_checkpoint(root_dir):\n",
    "    last_version = get_latest_version(root_dir)\n",
    "    checkpoints = sorted(glob.glob(f\"{last_version}/checkpoints/*\")) if last_version else []\n",
    "    return checkpoints[-1] if checkpoints else None\n",
    "\n",
    "def show_images(imgs, scale=2):\n",
    "    imgs = np.concatenate([\n",
    "        np.concatenate([\n",
    "            (255 * img.transpose((1, 2, 0))).round().clip(0, 255).astype(np.uint8)\n",
    "            for img in row\n",
    "        ], axis=1)\n",
    "        for row in imgs\n",
    "    ], axis=0)\n",
    "\n",
    "    display(Image.fromarray(imgs, \"RGB\").resize([scale * d for d in imgs.shape[:2][::-1]]))\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Named tensors and all their associated APIs are an experimental feature and subject to change.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-anthony",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Часть 1: Автоэнкодеры"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-firmware",
   "metadata": {},
   "source": [
    "Автоэнкодеры (autoencoder, AE, автокодировщик) - особый вид нейросетевых архитектур, состоящих из двух последовательных подсетей, называемых энкодером и декодером. Задача энкодера - понизить размерность входных данных, а задача декодера - восстановить исходные данные по выходу энкодера.\n",
    "\n",
    "Обучение автоэкодера заключается в минимизации \"ошибки рекострукции\" - расстояния между исходными входными данными и восстановленными декодером данными. Достаточно часто в качестве \"расстояния\" берется просто поэлементное среднеквадратичное отклонение, однако в общем случае можно минимизировать и любое другое расстояние.\n",
    "\n",
    "Если декодер успешно восстанавливает входные данные, то можно считать, что промежуточные низкоразмерные данные являются особым *представлением* исходных данных. Такое \"сжатое\" представление называется \"латентным\", а исходное представление данных - \"естественным\".\n",
    "\n",
    "Важно понимать, что понижение размерности возможно только за счет *избыточностей* в естественном представлении целевых данных. Например, цветные фотографии лиц знаменитостей можно естественно представить как элементы в $\\mathbb{R}^{H{\\times}W{\\times}3}$, однако данное представление очевидно избыточно. Подавляющее большинство элементов $\\mathbb{R}^{H{\\times}W{\\times}3}$ выглядит как случайный шум или как какие-то другие изображения (не лиц знаменитостей).\n",
    "\n",
    "С другой стороны, интуиция подсказывает нам, что пространство фотографий лиц знаменитостей не дискретно, а обладает некоторой \"локальностью\" (*небольшие* изменения пикселей фотографии не приводят к резкому изменению свойств фотографии). Отсюда следует логичное предположение, что латентное представление тоже будет обладать этим свойством."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-piano",
   "metadata": {},
   "source": [
    "![ae.png](https://docs.google.com/uc?export=download&id=1sCME120ySUHrW3_Gsi-9fHtJyoOaVsQ9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-lying",
   "metadata": {},
   "source": [
    "### Детали реализации\n",
    "\n",
    "Обычно в случае работы с изображениями, в качестве сети-энкодера используется какая-нибудь \"стандартная\" сверточная нейросеть, сжимающая входное изображения в \"плотный\" вектор признаков. Архитектура сети-декодера при этом зачастую составляет \"отраженную\" архитектуру энкодера, в которой все слои идут в обратном порядке и вместо постепенного уменьшение разрешения, происходит его увеличение.\n",
    "\n",
    "Для повышения разрешения изображения можно заменять слои по следующему принципу. <br/>\n",
    "Вместо [усреднения/пулинга (`AvgPool2d`)](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html) использовать [интерполяцию/масштабирование (`Upsample`)](https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html). <br/>\n",
    "Вместо [сверток (`Conv2d`)](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) с шагом (`stride > 1`) использовать [транспонированные свертки (`ConvTranspose2d`)](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html) с тем же шагом.\n",
    "\n",
    "Здесь и в последующих заданиях в качестве архитектур энкодера/декодера используется модифицированная версия ResNet-18.\n",
    "\n",
    "Внимательно рассмотрите код предоставленной реализации простого сверточного автоэнкодера. <br/>\n",
    "Особое внимание обратите на функции `forward` и `step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-professional",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebAAutoEncoder(CelebABoilerplate):\n",
    "    def __init__(self, first_conv=False, maxpool1=False, latent_dim=512, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = resnet18_encoder(self.hparams.first_conv, self.hparams.maxpool1)\n",
    "        self.decoder = resnet18_decoder(self.hparams.latent_dim, self.hparams.image_size, self.hparams.first_conv, self.hparams.maxpool1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_rec = self.decoder(z)\n",
    "        return z, x_rec\n",
    "\n",
    "    def step(self, batch, batch_idx):\n",
    "        kind = \"train\" if self.training else \"valid\"\n",
    "        x, _ = batch\n",
    "        z, x_rec = self.forward(x)\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        loss = F.mse_loss(x, x_rec)\n",
    "        self.log(f\"{kind}_loss\", loss, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def training_step(self, *args, **kwargs):\n",
    "        return self.step(*args, **kwargs)\n",
    "    \n",
    "    def validation_step(self, *args, **kwargs):\n",
    "        return self.step(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-convenience",
   "metadata": {},
   "source": [
    "Для экономии времени на семинаре вам предлагается загрузить заранее обученную версию модели. <br/>\n",
    "Также для справки приведен код, который был использован для обучения этой модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an AutoEncoder\n",
    "if False:\n",
    "    ae = CelebAAutoEncoder(image_size=56)\n",
    "    trainer = pl.Trainer(\n",
    "        auto_lr_find=True,\n",
    "        gpus=1,\n",
    "        log_gpu_memory=True,\n",
    "        callbacks=[pl.callbacks.EarlyStopping(\"valid_loss\", patience=2, verbose=True)],\n",
    "        default_root_dir=\"AutoEncoder\",\n",
    "        resume_from_checkpoint=get_latest_checkpoint(\"AutoEncoder\"),\n",
    "    )\n",
    "    # Tune the learning rate\n",
    "    trainer.tune(ae)['lr_find'].plot()\n",
    "    plt.show()\n",
    "    # Train the model\n",
    "    trainer.fit(ae)\n",
    "else:\n",
    "    ae = CelebAAutoEncoder.load_from_checkpoint(\"AutoEncoder.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-wilson",
   "metadata": {},
   "source": [
    "### Визуализация результатов\n",
    "\n",
    "Давайте рассмотрим результаты работы обученного автоэнкодера. Для этого визуализируем исходные и восстановленные изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-philippines",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_autoencoder_results(ae, num_images=10):\n",
    "    ae.train(False)\n",
    "    valid_ds = ae.get_dataset(\"valid\")\n",
    "\n",
    "    inps = []\n",
    "    outs = []\n",
    "    for i in np.random.choice(len(valid_ds), size=num_images):\n",
    "        inp, _ = valid_ds[i]\n",
    "        _, out, *_ = ae.forward(inp[np.newaxis])\n",
    "        out = out.squeeze(0).detach()\n",
    "        inps.append(inp.numpy())\n",
    "        outs.append(out.numpy())\n",
    "\n",
    "    show_images([inps, outs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-suspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_autoencoder_results(ae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-manor",
   "metadata": {},
   "source": [
    "**❓Ответьте на вопросы:** Похожи ли восстановленные изображения на исходные?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-above",
   "metadata": {},
   "source": [
    "### Свойства латентного пространства\n",
    "\n",
    "Также, давайте проверим нашу гипотезу о локальности латентного пространства. Для этого вычислим латентные представления для двух случайных изображений и линейно проинтерполируем несколько промежуточных латентных представлений. Декодируем и визуализируем эти латентные представления."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_autoencoder_interpolation(ae, num_images=10):\n",
    "    ae.train(False)\n",
    "    valid_ds = ae.get_dataset(\"valid\")\n",
    "\n",
    "    i0, i1 = np.random.choice(len(valid_ds), size=2)\n",
    "    img0, _ = valid_ds[i0]\n",
    "    img1, _ = valid_ds[i1]\n",
    "    z0, *_ = ae.forward(img0[np.newaxis])\n",
    "    z1, *_ = ae.forward(img1[np.newaxis])\n",
    "    \n",
    "    inps = [img0.numpy()] + (num_images-2) * [np.zeros(img0.shape)] + [img1.numpy()]\n",
    "    outs = []\n",
    "    for lerp in np.linspace(0, 1, num_images):\n",
    "        out = ae.decoder((1-lerp) * z0 + lerp * z1).squeeze(0).detach()\n",
    "        outs.append(out.numpy())\n",
    "\n",
    "    show_images([inps, outs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_autoencoder_interpolation(ae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-robinson",
   "metadata": {},
   "source": [
    "**❓Ответьте на вопросы:** Оправдалось ли наше предположение о локальности латентного пространства?\n",
    "\n",
    "### Генерация новых изображений с помощью автоэнкодера\n",
    "\n",
    "Возвращаясь к теме семинара, давайте подумаем, можно ли как-нибудь использовать получившееся латентное представление и его свойства для генерации новых изображений.\n",
    "\n",
    "Как было упомянуто ранее, нам известно, что множество \"фотографии лиц знаменитостей\" как подпространство $\\mathbb{R}^{H{\\times}W{\\times}3}$ составляет крайне малую долю из всех возможных изображений. Из-за этого, мы очевидно не можем просто сгенерировать матрицу случайных значений размера $H{\\times}W{\\times}3$ и ожидать, что это будет фотографией лица.\n",
    "\n",
    "С другой стороны, наше латентное пространство имеет сильно меньшую размерность, а значит разумно предположить, что множество \"фотографии лиц знаменитостей\" в нем будет более \"плотно\" расположено. Давайте попробуем визуализировать результаты декодирования для случайных латентных векторов.\n",
    "\n",
    "Для простоты, предположим также что латентные вектора состоят из нормально распределенных случайных величин. Вычислим среднее значение и среднеквадратичное отклонение элементов латентных векторов, сгенерируем с помощью этих параметров новые случайные латентные вектора и визуализируем их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(ae, num_samples=1000):\n",
    "    ae.train(False)\n",
    "    valid_ds = ae.get_dataset(\"valid\")\n",
    "\n",
    "    zs = []\n",
    "    for i in np.random.choice(len(valid_ds), size=num_samples):\n",
    "        inp, _ = valid_ds[i]\n",
    "        z, *_ = ae.forward(inp[np.newaxis])\n",
    "        out = z.squeeze(0).detach()\n",
    "        zs.append(out.numpy())\n",
    "\n",
    "    mean = np.mean(zs, axis=0)\n",
    "    std = np.std(zs, axis=0)\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def show_decode_random(ae, mean, std, num_images=10):\n",
    "    ae.train(False)\n",
    "\n",
    "    mean = torch.from_numpy(mean).to(ae.device)\n",
    "    std = torch.from_numpy(std).to(ae.device)\n",
    "    imgs = []\n",
    "    for i in range(num_images):\n",
    "        imgs.append([])\n",
    "        for k in range(num_images):\n",
    "            z = mean + std * torch.randn(ae.hparams.latent_dim).to(ae.device)\n",
    "            out = ae.decoder(z[np.newaxis]).squeeze(0).detach()\n",
    "            imgs[-1].append(out.numpy())\n",
    "\n",
    "    show_images(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = get_mean_std(ae)\n",
    "show_decode_random(ae, mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-hello",
   "metadata": {},
   "source": [
    "**❓Ответьте на вопросы:** Хорошие ли получились изображения? Если нет, попробуйте объяснить в чем проблема. Какое из наших предположений не оправдалось (о \"локальности\" латентного представления, о \"плотности\" латентного представления или о нормальности распределения латентных векторов)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-needle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up some memory for the next section\n",
    "del ae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-reynolds",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Часть 2: Вариационные автоэнкодеры"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-deadline",
   "metadata": {},
   "source": [
    "Вариационные автоэнкодеры (variational autoencoders, VAE) - модификация стандартных автоэнкодеров, нацеленная на улучшение свойств латентного представления с целью генерации правдоподобных новых данных.\n",
    "\n",
    "Как можно было догадаться из последнего задания, предложенный метод генерации новых данных с помощью стандартных автоэнкодеров работал плохо из-за того что не выполнялось одно из сделанных нами предположений. А именно, латентные вектора в стандартных автоэнкодерах **не** распределены нормально.\n",
    "\n",
    "Действительно, без каких либо ограничений на латентное представление нет оснований полагать, что элементы вектора будут иметь нужное нам распределение. Теперь, когда мы знаем, в чем была проблема стандартного автоэнкодера, рассмотрим как эту проблему предлагается решить в вариационных автоэнкодерах.\n",
    "\n",
    "Вместо того чтобы предсказывать латентный вектор напрямую, вариационный энкодер предсказывает параметры для распределения и затем из этого распределения сэмплируется случайный латентный вектор. Полученный случайный латентный вектор, как и в стандартном автоэнкодере, должен декодироваться в исходное изображение. Таким образом, латентные вектора гарантированно будут иметь нужное нам распределение (по построению)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-deputy",
   "metadata": {},
   "source": [
    "![vae.png](https://docs.google.com/uc?export=download&id=1N1yUxjB32gN2EYI3efKSseHyY3xjJNnI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-rabbit",
   "metadata": {},
   "source": [
    "Концептуально, идея вариационных автоэнкодеров получается достаточно простая, однако на практике есть ещё 3 важных нюанса, которые необходимо учесть для получения работающего решения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-portuguese",
   "metadata": {},
   "source": [
    "### $N$-мерность пространства латентных векторов\n",
    "\n",
    "Особо внимательные студенты могли заметить, что говоря о распределениях латентных представлений мы намеренно опускали одну важную деталь - латентные представления являются $N$-мерными векторами, а не скалярами.\n",
    "\n",
    "В частности, это значит что элементы этих векторов могут быть не независимо распределены, а \"нормальное распределение\" должно принять вид $\\mathcal{N}\\left(\\mu,\\,\\Sigma\\right)$, где $\\mu \\in \\mathbb{R}^N$ - вектор сдвига, а $\\Sigma \\in \\mathbb{R}^{N{\\times}N}$ - ковариационная матрица. Использование произвольной ковариационной матрицы значительно усложняет дальнейшие выводы, поэтому обычно вводится дополнительное ограничение $\\Sigma = \\mathrm{I}\\sigma$, где $\\sigma \\in \\mathbb{R}^N_+$ - вектор среднеквадратичных отклонений.\n",
    "\n",
    "Если исходная матрица $\\Sigma$ была диагонализируема, то данное ограничение даже не приводит к потере общности (не диагональный множитель можно \"занести\" в первый линейный слой в декодере)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-father",
   "metadata": {},
   "source": [
    "### Регуляризация распределения\n",
    "\n",
    "В вариационном автоэнкодере мы заменили предсказание конкретных латентных значений на моделирование их распределения. Однако в текущей версии нашего алгоритма, энкодер может \"обойти\" наше требование просто предсказывая очень большие значения $\\mu$, либо очень маленькие значения $\\sigma$. В результате предсказанные распределения будут вести себя почти как обыкновенные \"точечные\" предсказания в стандартном автоэнкодере.\n",
    "\n",
    "Чтобы вырождения распределений не происходило, необходимо регуляризовать предсказанные энкодером параметры распределений."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-works",
   "metadata": {},
   "source": [
    "![regularization.png](https://docs.google.com/uc?export=download&id=1jR-TzxqG5v1cgsTOpj6XTgPAPMpRCr4S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-consideration",
   "metadata": {},
   "source": [
    "Для регуляризации параметров распределений в вариационном автоэнкодере, предлагается добавить в функцию потерь специальное слагаемое, измеряющее степень близости предсказанного распределения к стандартному.\n",
    "\n",
    "Как вам известно, [дивергенция Кульбака — Лейблера](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D1%81%D1%81%D1%82%D0%BE%D1%8F%D0%BD%D0%B8%D0%B5_%D0%9A%D1%83%D0%BB%D1%8C%D0%B1%D0%B0%D0%BA%D0%B0_%E2%80%94_%D0%9B%D0%B5%D0%B9%D0%B1%D0%BB%D0%B5%D1%80%D0%B0) задает аналог \"расстояния\" или меры удаленности одного распределения от другого.\n",
    "\n",
    "В данном задании, вам предлагается аналитически вывести формулу для вычисления расстояния Кульбака — Лейблера между нормальным распределением с параметрами $\\mu$, $\\sigma$ (скаляры) и стандартным нормальным распределением (с параметрами $0$, $1$).\n",
    "\n",
    "$$\n",
    "\\large\\mathrm{KL}\\left(\\mathcal{N}\\left(\\mu, \\sigma^2\\right),\\,\\mathcal{N}\\left(0, 1\\right)\\right)\n",
    "$$\n",
    "\n",
    "В ходе аналитического вывода, вам могут понадобиться следующие формулы:\n",
    "- Определение KL-дивергенции\n",
    "  $$\\large\\mathrm{KL}\\left(p,\\,q\\right) = \\int_{-\\infty}^{+\\infty} p(x) \\log\\left(\\frac{p(x)}{q(x)}\\right) \\mathrm{d}x$$\n",
    "- Формула плотности нормального распределения\n",
    "  $$\\large\\mathcal{N}\\left(\\mu, \\sigma^2\\right) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}{\\left(\\frac{x-\\mu}{\\sigma}\\right)}^2}$$\n",
    "- Формула математического ожидания абсолютно непрерывной случайной величины (и её преобразований)\n",
    "  $$\\large\\mathbb{E}\\left[f(x)\\right] = \\int_{-\\infty}^{+\\infty} f(x) p(x) \\mathrm{d}x$$\n",
    "  где $p(x)$ - плотность распределения непрерывной случайной величины $x$, а $f(x)$ - любая борелевская функция\n",
    "- Две формулы дисперсии случайной величины\n",
    "  $$\\large\\mathbb{D}x = \\mathbb{E}\\left[\\left(x - \\mathbb{E}x\\right)^2\\right] \\qquad \\small\\text{и} \\large\\qquad \\mathbb{D}x = \\mathbb{E}\\left[x^2\\right] - \\left(\\mathbb{E}x\\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-nursing",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Используя аналитически выведенную формулу, реализуйте функцию `normal_distribution_kl_divergence`. <br/>\n",
    "**Не** используйте при этом классы/функции из модуля `torch.distributions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-surveillance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_distribution_kl_divergence(mu, sigma):\n",
    "    # \\/ Your code here \\/\n",
    "    # /\\ Your code here /\\\n",
    "    return kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-plaintiff",
   "metadata": {},
   "source": [
    "Проверьте вашу реализацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_normal_distribution_kl_divergence():\n",
    "    mu_value = torch.randn(123, 321)\n",
    "    sigma_value = torch.exp(torch.randn(123, 321))\n",
    "\n",
    "    # Compute reference values\n",
    "    mu_ref = torch.autograd.Variable(mu_value, requires_grad=True)\n",
    "    sigma_ref = torch.autograd.Variable(sigma_value, requires_grad=True)\n",
    "    kl_loss_ref = torch.distributions.kl_divergence(\n",
    "        torch.distributions.Normal(mu_ref, sigma_ref),\n",
    "        torch.distributions.Normal(torch.zeros_like(mu_ref), torch.ones_like(sigma_ref)),\n",
    "    ).mean(axis=0).sum()\n",
    "    kl_loss_ref.backward()\n",
    "    mu_ref_grad = mu_ref.grad\n",
    "    sigma_ref_grad = sigma_ref.grad\n",
    "\n",
    "    # Check student implementation\n",
    "    mu = torch.autograd.Variable(mu_value, requires_grad=True)\n",
    "    sigma = torch.autograd.Variable(sigma_value, requires_grad=True)\n",
    "    kl_loss = normal_distribution_kl_divergence(mu, sigma)\n",
    "\n",
    "    # The function must produce the same values as the library implementation\n",
    "    assert torch.allclose(kl_loss, kl_loss_ref)\n",
    "\n",
    "    # The function must be differentiable\n",
    "    assert mu.grad is None\n",
    "    assert sigma.grad is None\n",
    "    kl_loss.backward()\n",
    "    assert torch.allclose(mu.grad, mu_ref_grad)\n",
    "    assert torch.allclose(sigma.grad, sigma_ref_grad)\n",
    "\n",
    "test_normal_distribution_kl_divergence()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-kingston",
   "metadata": {},
   "source": [
    "### Репараметризация случайных величин\n",
    "\n",
    "Последний нюанс в реализации вариационного автоэнкодера кроется в самом процессе сэмплирования случайных величин. На данный момент, прямой проход для вариационного автоэнкодера может быть расписан следующим образом:\n",
    "\n",
    "1. из изображения $\\mathbf{x}$ извлекается вектор признаков $\\varphi$ <br/>\n",
    "    $\\varphi = \\mathrm{ENC}\\left(\\mathbf{x}\\right)$\n",
    "2. с помощью извлеченных признаков вычисляются параметры распределения <br/>\n",
    "    $\\mu = f_{\\mu}\\left(\\varphi\\right)$ <br/>\n",
    "    $\\sigma = f_{\\sigma}\\left(\\varphi\\right)$\n",
    "3. латентный вектор сэмплируется из заданного распределения <br/>\n",
    "    $\\mathbf{z} \\sim \\mathcal{N}\\left(\\mu,\\,\\sigma^2\\right)$\n",
    "4. латентный вектор декодируется в реконструкцию изображения $\\mathbf{x}'$ <br/>\n",
    "    $\\mathbf{x}' = \\mathrm{DEC}\\left(\\mathbf{z}\\right)$\n",
    "\n",
    "Однако такая формулировка прямого прохода не позволяет обучать энкодер, потому что операция сэмплирования случайной величины из распределения \"$\\sim$\" в шаге 3 - не дифференцируема. Чтобы убрать не дифференцируемую операцию \"$\\sim$\" с пути градиента, используется прием репараметризации случайных величин.\n",
    "\n",
    "Вместо того чтобы напрямую сэмплировать $\\mathbf{z} \\sim \\mathcal{N}\\left(\\mu,\\,\\sigma^2\\right)$, можно сэмплировать $\\mathbf{z}'$ из стандартного нормального распределения $\\mathbf{z}' \\sim \\mathcal{N}\\left(0,\\,1\\right)$, а затем [привести](https://ru.wikipedia.org/wiki/%D0%9D%D0%BE%D1%80%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%B5_%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5#%D0%9C%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5_%D0%BD%D0%BE%D1%80%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D1%85_%D0%BF%D1%81%D0%B5%D0%B2%D0%B4%D0%BE%D1%81%D0%BB%D1%83%D1%87%D0%B0%D0%B9%D0%BD%D1%8B%D1%85_%D0%B2%D0%B5%D0%BB%D0%B8%D1%87%D0%B8%D0%BD) полученное случайное значение $\\mathbf{z}'$ к случайному значению $\\mathbf{z}$ с интересующими нас параметрами $\\mu$ и $\\sigma$. После данной замены, $\\mathbf{z}$ должно выражаться как дифференцируемая функция от $\\mathbf{z}'$, $\\sigma$ и $\\mu$.\n",
    "\n",
    "Реализуйте функцию `differentiable_sample_normal`, в которой производится описанное вычисление.\n",
    "Для сэмплирования случайных величин из стандартного нормального распределения, используйте функцию `torch.randn_like`.\n",
    "Обратите внимание, что на вход `differentiable_sample_normal` передается логарифм `sigma` (это сделано для численной стабильности и чтобы `sigma` не могла принимать отрицательные значения)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-beach",
   "metadata": {},
   "outputs": [],
   "source": [
    "def differentiable_sample_normal(mu, log_sigma):\n",
    "    # \\/ Your code here \\/\n",
    "    # /\\ Your code here /\\\n",
    "    return sample, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-financing",
   "metadata": {},
   "source": [
    "Проверьте вашу реализацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-housing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_differentiable_sample_normal():\n",
    "    current_seed = torch.seed()\n",
    "    random_values = torch.randn(100)\n",
    "    zeros_mu = torch.autograd.Variable(torch.zeros_like(random_values), requires_grad=True)\n",
    "    zeros_log_sigma = torch.autograd.Variable(torch.zeros_like(random_values), requires_grad=True)\n",
    "\n",
    "    torch.manual_seed(current_seed)\n",
    "    sample, sigma = differentiable_sample_normal(zeros_mu, zeros_log_sigma)\n",
    "\n",
    "    # The random values must be obtained from torch.randn/torch.randn_like\n",
    "    assert torch.allclose(sample, random_values)\n",
    "    # log_sigma=0 -> sigma=1\n",
    "    assert torch.allclose(sigma, torch.ones_like(zeros_log_sigma))\n",
    "    # The function must be differentiable\n",
    "    assert zeros_mu.grad is None\n",
    "    assert zeros_log_sigma.grad is None\n",
    "    sample.sum().backward()\n",
    "    assert torch.allclose(zeros_mu.grad, torch.ones_like(zeros_mu))\n",
    "    assert torch.allclose(zeros_log_sigma.grad, random_values)\n",
    "\n",
    "test_differentiable_sample_normal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-warner",
   "metadata": {},
   "source": [
    "### Детали реализации\n",
    "\n",
    "Внимательно рассмотрите код предоставленной реализации сверточного вариационного автоэнкодера.\n",
    "Особое внимание обратите на функции `forward` и `step`.\n",
    "Посмотрите, как здесь используются реализованные вами функции `normal_distribution_kl_divergence` и `differentiable_sample_normal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebAVariationalAutoEncoder(CelebAAutoEncoder):\n",
    "    def __init__(self, kl_coeff=0.00005, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.save_hyperparameters()\n",
    "        self.f_mu = torch.nn.Linear(self.hparams.latent_dim, self.hparams.latent_dim)\n",
    "        self.f_log_sigma = torch.nn.Linear(self.hparams.latent_dim, self.hparams.latent_dim)\n",
    "        # initialize params so that self.f_mu(X) == X\n",
    "        torch.nn.init.eye_(self.f_mu.weight)\n",
    "        torch.nn.init.zeros_(self.f_mu.bias)\n",
    "        # initialize params so that self.f_log_sigma(X) == -1\n",
    "        torch.nn.init.zeros_(self.f_log_sigma.weight)\n",
    "        torch.nn.init.constant_(self.f_log_sigma.bias, -1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        \n",
    "        mu = self.f_mu(features)\n",
    "        log_sigma = self.f_log_sigma(features)\n",
    "\n",
    "        if self.training:\n",
    "            z, sigma = differentiable_sample_normal(mu, log_sigma)\n",
    "        else:\n",
    "            # During validation evaluate\n",
    "            # encoder/decoder without noise\n",
    "            z, sigma = mu, torch.exp(log_sigma)\n",
    "        \n",
    "        return z, self.decoder(z), mu, sigma\n",
    "\n",
    "    def step(self, batch, batch_idx):\n",
    "        kind = \"train\" if self.training else \"valid\"\n",
    "        x, _ = batch\n",
    "        z, x_rec, mu, sigma = self.forward(x)\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        rec_loss = F.mse_loss(x, x_rec)\n",
    "        self.log(f\"{kind}_rec_loss\", rec_loss, prog_bar=True)\n",
    "        \n",
    "        # KL loss\n",
    "        kl_loss = self.hparams.kl_coeff * normal_distribution_kl_divergence(mu, sigma)\n",
    "        self.log(f\"{kind}_kl_loss\", kl_loss, prog_bar=True)\n",
    "        \n",
    "        loss = rec_loss + kl_loss\n",
    "        self.log(f\"{kind}_loss\", loss, prog_bar=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-membrane",
   "metadata": {},
   "source": [
    "Для экономии времени на семинаре вам предлагается загрузить заранее обученную версию модели. <br/>\n",
    "Также для справки приведен код, который был использован для обучения этой модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VariationalAutoEncoder\n",
    "if False:\n",
    "    # Init model with pretrained encoder/decoder from previous part\n",
    "    vae = CelebAVariationalAutoEncoder.load_from_checkpoint(get_latest_checkpoint(\"AutoEncoder\"), strict=False)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        auto_lr_find=True,\n",
    "        gpus=1,\n",
    "        log_gpu_memory=True,\n",
    "        callbacks=[pl.callbacks.EarlyStopping(\"valid_loss\", patience=1, verbose=True)],\n",
    "        default_root_dir=\"VariationalAutoEncoder\",\n",
    "    )\n",
    "    # Tune the learning rate\n",
    "    trainer.tune(vae)['lr_find'].plot()\n",
    "    plt.show()\n",
    "    # Fine-tune the model\n",
    "    trainer.fit(vae)\n",
    "else:\n",
    "    vae = CelebAVariationalAutoEncoder.load_from_checkpoint(\"VariationalAutoEncoder.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-killing",
   "metadata": {},
   "source": [
    "### Визуализация результатов\n",
    "\n",
    "Давайте рассмотрим результаты работы обученного вариационного автоэнкодера.\n",
    "\n",
    "Сравните полученные результаты с результатами из прошлой части."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-curtis",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_autoencoder_results(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-arkansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_autoencoder_interpolation(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-tokyo",
   "metadata": {},
   "source": [
    "**❓Ответьте на вопросы:** Как изменилось качество реконструкции? Осталось ли у латентного представления свойство локальности?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-honduras",
   "metadata": {},
   "source": [
    "### Генерация новых изображений с помощью вариационного автоэнкодера\n",
    "\n",
    "Давайте попробуем сгенерировать новые изображения тем же способом, что и в прошлой части."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-dress",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = get_mean_std(vae)\n",
    "show_decode_random(vae, mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-sterling",
   "metadata": {},
   "source": [
    "**❓Ответьте на вопросы:** Получилось ли с помощью вариационного автоэнкодера убрать артефакты при генерации новых изображений? Что вы можете сказать о качестве сгенерированных изображений? Как вы думаете, почему автоэнкодеры и вариационные автоэнкодеры генерируют слегка размытые изображения? Что можно сделать, чтобы получать более изображения с более резкими границами и текстурами?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-macintosh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up some memory for the next section\n",
    "del vae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-hamilton",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Часть 3: Генеративно-состязательные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-sandwich",
   "metadata": {},
   "source": [
    "Хотя исторически идея генеративно-состязательных сетей была придумана независимо от вариационных автоэнкодеров, в данном задании мы в образовательных целях будем рассматривать генеративно-состязательные сети как дальнейшее развитие вариационных автоэнкодеров.\n",
    "\n",
    "В конце прошлой части мы задали вопрос <i>\"Почему автоэнкодеры и вариационные автоэнкодеры генерируют слегка размытые изображения?\"</i>. Одно из возможных объяснений заключается в том, что при вычислении ошибки реконструкции входного изображения мы использовали попиксельное расстояние между изображениями. Таким образом, наша ошибка реконструкции никак не учитывает пространственную информацию и как следствие вынуждает декодер \"размывать\" детали, в пространственном расположении которых декодер не уверен.\n",
    "\n",
    "Чтобы лучше понять, почему использование попиксельное расстояние между изображениями приводит к таким результатам, давайте рассмотрим конкретный пример. На следующей иллюстрации все 5 изображений (b-f) имеют одинаковое попиксельное расстояние до исходного изображения (а). Это противоречит нашему \"интуитивному\" пониманию о схожести этих изображений. Так, например, изображение (b) получено смещением на несколько пикселей и для человека почти не отличимо от оригинала, в то время как изображения (e) и (f) очевидно сильно отличаются от оригинала."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-collect",
   "metadata": {},
   "source": [
    "![mse.png](https://docs.google.com/uc?export=download&id=1F8M4H_4lfkGwkJp3hNvl80yx3npmpVwb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-curve",
   "metadata": {},
   "source": [
    "Теперь, когда мы обратили внимание на этот недостаток ошибки реконструкции, логично было бы попробовать заменить ошибку реконструкции на какую-нибудь другую функцию ошибки. В идеале, нам бы хотелось, чтобы эта новая функция ошибки обладала \"человеческой интуицией\" о том, какие особенности изображения важны, а какие - нет. Конечно, такой функции в аналитическом виде скорее всего не существует, однако мы можем получить схожий результат, заменив \"человеческую интуицию\" на \"статистическое правдоподобие\".\n",
    "\n",
    "Если бы у нас была дифференцируемая функция, которая для произвольного изображения могла бы оценить правдоподобие этого изображения или другими словами - вероятноясть найти такое изображение в эталонной выборке, то мы бы могли использовать данную функцию при обучении декодера (цель декодера - максимизировать правдоподобие сгенерированного изображения). К счастью, мы уже знаем, как можно получить такую функцию - обучить бинарный классификатор, отличающий настоящие изображения от \"поддельных\", сгенерированных.\n",
    "\n",
    "Таким образом вместо $\\min\\ {\\left\\lVert\\mathrm{DEC}\\left(\\mathbf{z}\\right) - \\mathbf{x}\\right\\rVert}^2_2$ мы будем искать $\\max\\ \\mathbf{p}\\left(\\mathrm{DEC}\\left(\\mathbf{z}\\right)\\right)$, где $\\mathbf{p}$ - вероятность принадлежности к эталонному датасету. Предсказывать $\\mathbf{p}$ будем с помощью сети-энкодера с дополнительным линейным слоем и сигмоидальной функцией активации (как в обычной задаче бинарной классификации).\n",
    "\n",
    "После этой замены, мы можем дополнительно слегка упростить нашу архитектуру. Заметим, что в отличие от ошибки реконструкции, максимизация правдоподобия не сопоставляет сгенерированное изображение с каким-то конкретным эталонным изображением $\\mathbf{x}$. Это значит, что оптимальные параметры для распределения $\\mathbf{z}$ больше не зависят от входного изображения. Учитывая их регуляризацию с помощью дивергенции Кульбака — Лейблера, мы можем зафиксировать оптимальное распределение $\\mathbf{z} \\sim \\mathcal{N}\\left(0,\\,1\\right)$, не зависящее от $\\mathbf{x}$.\n",
    "\n",
    "В результате мы \"поменяли местами\" энкодер и декодер. Теперь первая сеть в модели - декодер, который на основании случайного шума учится генерировать правдоподобные изображения, а вторая сеть в модели - энкодер, который учится отличать \"поддельные\" сгенерированные изображения от настоящих примеров из датасета.\n",
    "\n",
    "В данной конфигурации, декодер принято называть **генератором**, а энкодер - **дискриминатором**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-collins",
   "metadata": {},
   "source": [
    "![gan.png](https://docs.google.com/uc?export=download&id=1-CIxil5VPgNg0W7DVqTmdUUZ3XCAMxFf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-basis",
   "metadata": {},
   "source": [
    "Генеративно-состязательные сети (Generative Adversarial Network, GAN) - вид нейросетевых архитектур, состоящих из двух последовательных сетей, называемых генератором и дискриминатором. В генеративно-состязательных сетях, генератор пытается создать правдоподобно выглядящие данные, а дискриминатор пытается отличить настоящие данные от \"поддельных\", созданных генератором."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-postage",
   "metadata": {},
   "source": [
    "### Состязательная природа обучения генератора и дискриминатора\n",
    "\n",
    "Важная деталь генеративно-состязательных сетей, которая пока была упомянута только вскользь - это состязательная природа процесса обучения таких сетей. Как можно заметить, в выше описанной архитектуре сеть-генератор и сеть-дискриминатор на самом деле имеют разные, конфликтующие цели. Чем лучше работает генератор, тем сложнее будет дискриминатору отличить поддельные изображения от настоящих. Именно из-за данной особенности, эта архитектура называется состязательной (adversarial).\n",
    "\n",
    "Более того, в прошлом разделе мы сказали что для оценки правдоподобия изображений мы \"обучим бинарный классификатор\" (дискриминатор), однако на практике мы не можем *заранее* обучить и зафиксировать дискриминатор. Если зафиксировать дискриминатор, то из-за состязательной природы задачи, генератор моментально переобучится под этот конкретный зафиксированный дискриминатор.\n",
    "\n",
    "Чтобы решить данную проблему, нам нужно попеременно обучать и генератор и дискриминатор. Тогда обе эти сети будут вынуждены постоянно улучшать свои результаты в своеобразной \"гонке вооружений\". В итоге алгоритм обучения генеративно-состязательной сети может быть подытожен так:\n",
    "\n",
    "1. Шаг обучения дискриминатора (веса генератора заморожены)\n",
    "    - Выбирается батч эталонных изображений $\\mathbf{x}$\n",
    "    - Генерируется батч поддельных изображений $\\mathbf{x}' = \\mathrm{GEN}\\left(\\mathbf{z}\\right)$ (для случайных $\\mathbf{z} \\in \\mathcal{N}\\left(0, 1\\right)$)\n",
    "    - Дискриминатор предсказывает $\\mathbf{p} = \\mathrm{DIS}\\left(\\mathbf{x}\\right)$ и $\\mathbf{p}' = \\mathrm{DIS}\\left(\\mathbf{x}'\\right)$\n",
    "    - Происходит обратное распространение ошибки бинарной кросс-энтропии до весов дискриминатора. <br/>\n",
    "      При вычислении кросс-энтропии для $\\mathbf{p}$ эталонная метка - $1$ (настоящее изображение), а для $\\mathbf{p}'$ эталонная метка - $0$ (подделка).\n",
    "\n",
    "<br/>\n",
    "\n",
    "2. Шаг обучения генератора (веса дискриминатора заморожены)\n",
    "    - Генерируется батч поддельных изображений $\\mathbf{x}' = \\mathrm{GEN}\\left(\\mathbf{z}\\right)$ (для случайных $\\mathbf{z} \\in \\mathcal{N}\\left(0, 1\\right)$)\n",
    "    - Дискриминатор предсказывает $\\mathbf{p}' = \\mathrm{DIS}\\left(\\mathbf{x}'\\right)$\n",
    "    - Происходит обратное распространение ошибки бинарной кросс-энтропии до весов генератора. <br/>\n",
    "      При вычислении кросс-энтропии для $\\mathbf{p}'$ эталонная метка - $1$ (настоящее изображение).\n",
    "\n",
    "Обратите внимание, что эталонные метки для $\\mathbf{p}'$ при оптимизации дискриминатора и генератора - разные. Это - крайне важная деталь для понимания динамики обучения генеративно-состязательных моделей. Из-за того что дискриминатор и генератор оптимизируют разные (в каком-то смысле даже противоположные) функции ошибки, процесс обучения генеративно-состязательных моделей **нельзя считать градиентным спуском**.\n",
    "\n",
    "Многие теоремы, утверждения и интуиции, которые верны для обычных нейросетевых моделей (обучаемых градиентным спуском) не верны для генеративно-состязательных моделей. Например, при обучении градиентным спуском, мы привыкли, что по мере обучения значение функции потерь будет постепенно уменьшаться, пока не достигнет локально оптимального значения после чего модель либо \"заканчивает\" свое обучения, либо начинает переобучаться. При этом если функция потерь стала увеличиваться, то значит модель расходится.\n",
    "\n",
    "Для генеративно-состязательных моделей, функция потерь может уменьшаться, увеличиваться или оставаться неизменной в процессе обучения и однозначно судить о статусе обучения по динамике значения функции потерь (или даже отдельных слагаемых в функции потерь) нельзя. Для генеративно-состязательных моделей, значение функции потерь отражает не процесс схождения модели к оптимальному решению, а **баланс** между генератором и дискриминатором. Например, если дискриминатор и генератор становятся лучше с одинаковой скоростью, то среднее значение функции потерь не будет изменяться.\n",
    "\n",
    "Для генеративно-состязательных моделей, \"оптимальным\" является решение при котором распределение $\\mathrm{GEN}\\left(\\mathbf{z}\\right)$ идеально повторяет истинное распределение $\\mathbf{x}$ и соответственно дискриминатор не может отличить настоящие данные от генерируемых $\\mathrm{DIS}\\left(\\mathbf{x}\\right) = \\mathrm{DIS}\\left(\\mathbf{x}'\\right) = \\frac{1}{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-colleague",
   "metadata": {},
   "source": [
    "### Детали реализации\n",
    "\n",
    "Внимательно рассмотрите код предоставленной реализации сверточной генеративно-состязательной сети. <br/>\n",
    "Особое внимание обратите на функции `generator`, `discriminator`, `forward` и `step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-athletics",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebAGenerativeAdversarialNetwork(CelebAAutoEncoder):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.save_hyperparameters()\n",
    "        self.discriminator_fc = torch.nn.Linear(self.hparams.latent_dim, 1)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        dis_params = list(gan.encoder.parameters()) + list(gan.discriminator_fc.parameters())\n",
    "        gen_params = list(gan.decoder.parameters())\n",
    "        dis = torch.optim.Adam(dis_params, lr=self.hparams.lr)\n",
    "        gen = torch.optim.Adam(gen_params, lr=self.hparams.lr)\n",
    "        return [dis, gen], []\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.generator(*args, **kwargs)\n",
    "\n",
    "    def generator(self, z=None, num_samples=None):\n",
    "        if z is None:\n",
    "            z = torch.randn(num_samples, self.hparams.latent_dim, device=self.device)\n",
    "        else:\n",
    "            assert num_samples is None\n",
    "        # sigmoid restricts the output to range [0,1]\n",
    "        x_gen = torch.sigmoid(self.decoder(z))\n",
    "        return z, x_gen\n",
    "\n",
    "    def discriminator(self, x):\n",
    "        features = self.encoder(x)\n",
    "        logprob = self.discriminator_fc(features).squeeze(1)\n",
    "        # sigmoid(logprob)=0 means \"fake image\"\n",
    "        # sigmoid(logprob)=1 means \"real image\"\n",
    "        return logprob\n",
    "\n",
    "    def step(self, batch, batch_idx, optimizer_idx=None):\n",
    "        kind = \"train\" if self.training else \"valid\"\n",
    "        if optimizer_idx is None:\n",
    "            assert not self.training\n",
    "            # Manually run both generator and discriminator paths during the validation step\n",
    "            total_loss = self.step(batch, batch_idx, 0) + self.step(batch, batch_idx, 1)\n",
    "            self.log(f\"{kind}_loss\", total_loss, prog_bar=True)\n",
    "            return total_loss\n",
    "\n",
    "        # Generate fake images\n",
    "        x_real, _ = batch\n",
    "        _, x_fake = self.generator(num_samples=x_real.shape[0])\n",
    "\n",
    "        loss = []\n",
    "        if optimizer_idx == 0:\n",
    "            # Discriminator \n",
    "            kind += \"_dis\"\n",
    "\n",
    "            # Don't pass gradient through the generator\n",
    "            # when optimizing the discriminator\n",
    "            x_fake = x_fake.detach()\n",
    "\n",
    "            # Prepare ground truth data\n",
    "            gt_real = torch.ones(x_real.shape[0], device=self.device)\n",
    "            gt_fake = torch.zeros(x_fake.shape[0], device=self.device)\n",
    "\n",
    "            # Forward only discriminator with real data\n",
    "            logprob_real = self.discriminator(x_real)\n",
    "\n",
    "            # Loss component for real images (discriminator only)\n",
    "            real_loss = F.binary_cross_entropy_with_logits(logprob_real, gt_real)\n",
    "            self.log(f\"{kind}_real_loss\", real_loss, prog_bar=True)\n",
    "            loss.append(real_loss)\n",
    "        elif optimizer_idx == 1:\n",
    "            # Generator\n",
    "            kind += \"_gen\"\n",
    "    \n",
    "            # Prepare ground truth data\n",
    "            gt_fake = torch.ones(x_fake.shape[0], device=self.device)\n",
    "        else:\n",
    "            raise RuntimeError()\n",
    "\n",
    "        # Loss component for fake images (generator and discriminator)\n",
    "        # Note that during the generator optimization pass gt_fake = 1\n",
    "        # while during the discriminator optimization pass gt_fake = 0\n",
    "        logprob_fake = self.discriminator(x_fake)\n",
    "        fake_loss = F.binary_cross_entropy_with_logits(logprob_fake, gt_fake)\n",
    "        self.log(f\"{kind}_fake_loss\", fake_loss, prog_bar=True)\n",
    "        loss.append(fake_loss)\n",
    "                        \n",
    "        loss = sum(loss)\n",
    "        self.log(f\"{kind}_loss\", loss, prog_bar=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-maintenance",
   "metadata": {},
   "source": [
    "Для экономии времени на семинаре вам предлагается загрузить заранее обученную версию модели. <br/>\n",
    "Также для справки приведен код, который был использован для обучения этой модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-devil",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GenerativeAdversarialNetwork\n",
    "if False:\n",
    "    gan = CelebAGenerativeAdversarialNetwork(image_size=56)\n",
    "    \n",
    "    # Init model with pretrained encoder/decoder from previous part\n",
    "    vae = CelebAVariationalAutoEncoder.load_from_checkpoint(get_latest_checkpoint(\"VariationalAutoEncoder\"))\n",
    "    gan.encoder = vae.encoder\n",
    "    gan.decoder = vae.decoder\n",
    "    del vae\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        # auto_lr_find=True,  # Doesn't work with multiple optimizers\n",
    "        max_epochs=50,\n",
    "        gpus=1,\n",
    "        log_gpu_memory=True,\n",
    "        callbacks=[pl.callbacks.ModelCheckpoint(every_n_epochs=1, save_top_k=-1, verbose=True)],\n",
    "        default_root_dir=\"GenerativeAdversarialNetwork\",\n",
    "        resume_from_checkpoint=get_latest_checkpoint(\"GenerativeAdversarialNetwork\"),\n",
    "    )\n",
    "    # Fine-tune the model\n",
    "    trainer.fit(gan)\n",
    "else:\n",
    "    gan = CelebAGenerativeAdversarialNetwork.load_from_checkpoint(\"GenerativeAdversarialNetwork.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-segment",
   "metadata": {},
   "source": [
    "### Генерация новых изображений с помощью генеративно-состязательной сети\n",
    "\n",
    "Давайте рассмотрим результаты работы обученной генеративно-состязательной сети.\n",
    "\n",
    "Обратите внимание, что в отличие от автоэнкодеров, генеративно-состязательные сети моделируют только отображение из латентного пространства в пространство изображений. Это значит, что при визуализации результатов работы генеративно-состязательных сетей мы не можем сравнить сгенерированное изображение с \"эталоном\".\n",
    "\n",
    "Визуализируем несколько случайных генерируемых изображений и сравним их с изображениями сгенерированными при помощи вариационного автоэнкодера в прошлой части."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_generate_random(gan, num_images=10):\n",
    "    gan.train(False)\n",
    "\n",
    "    _, imgs = gan.generator(num_samples=num_images * num_images)\n",
    "    imgs = imgs.detach().numpy()\n",
    "    imgs = imgs.reshape((num_images, num_images) + imgs.shape[1:])\n",
    "\n",
    "    show_images(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-algebra",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_generate_random(gan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-sweden",
   "metadata": {},
   "source": [
    "**❓Ответьте на вопросы:** Получилось ли с помощью генеративно-состязательной сети сделать генерируемые изображения менее размытыми? Изменилась ли при этом \"реалистичность\" генерируемых лиц? Вы можете объяснить, почему это происходит? Что можно сделать, чтобы исправить данную проблему?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-driving",
   "metadata": {},
   "source": [
    "**❓Ответьте на вопросы:** Если внимательно посмотреть на генерируемые изображения, то можно заметить, что некоторые лица встречаются несколько раз (с небольшими изменениями). Вы можете объяснить, почему это происходит? Что можно сделать, чтобы исправить данную проблему?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-belarus",
   "metadata": {},
   "source": [
    "### Бонус: интерполяция между генерируемыми изображениями\n",
    "\n",
    "Хотя мы и не знаем \"эталонные\" изображения для каждого латентного вектора, мы все еще можем попробовать линейно интерполироваться между двумя случайными векторами в латентном пространстве. Посмотрите на результаты интерполяции для нескольких пар случайных латентных векторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-exhaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_gan_interpolation(gan, num_images=10):\n",
    "    gan.train(False)\n",
    "\n",
    "    z0 = torch.randn(num_images, gan.hparams.latent_dim, device=gan.device)\n",
    "    z1 = torch.randn(num_images, gan.hparams.latent_dim, device=gan.device)\n",
    "    \n",
    "    outs = []\n",
    "    for lerp in np.linspace(0, 1, num_images):\n",
    "        z = (1-lerp) * z0 + lerp * z1\n",
    "        _, out = gan.generator(z=z)\n",
    "        out = out.detach()\n",
    "        outs.append(out.numpy())\n",
    "\n",
    "    show_images(np.array(outs).swapaxes(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-turkish",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_gan_interpolation(gan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-arctic",
   "metadata": {},
   "source": [
    "**❓Ответьте на вопросы:** Как вам кажется, осталось ли у латентного представления свойство локальности? Как вы можете объяснить данный эффект?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
